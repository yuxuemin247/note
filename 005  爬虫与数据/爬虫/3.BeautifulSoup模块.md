# BeautifulSoup模块

## 简介

```python
beautifulSoup模块
beautifulSoup是一个第三方模块  专门用于解析XML文档,解析网页,并提供定位内容的便捷欸接口,HTML代码补全
目前最稳定的是bs4 版本
pip install bs4
pip install lxml
```

## 遍历文档树

```python
from bs4 import BeautifulSoup

# 要解析的文档内容
html_doc = """
<html>
<head><title>The Dormouse's story</title></head>
<body class="b a c">
<p class="story">
<ssss>hhhh</ssss>
Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
"""
# 第一个参数为要解析的文档数据
soup = BeautifulSoup(html_doc,'lxml')
# tag = soup.body
# print(type(tag))
# print(tag.name)
# print(tag.text)
# print(tag.attrs)

# 使用点语法查找标签，只能找到第一个名字匹配的标签
# tag = soup.a
# print(tag.attrs.get('href'))

# 嵌套选择
# print(soup.p.a.text)

# 获取子节点
# print(list(soup.p.children))
# 返回一个迭代器
# for i in soup.head.children:
#     print(i)

# print(soup.p.contents)
# 返回一个列表
# for i in soup.head.contents:
#     print(i)

# 获取父标签
# print(soup.p.parent)

# 获取所有的父辈标签
# print(list(soup.p.parents))
# for i in soup.p.parents:
#     print(i.name)

# print(list(soup.p.descendants))
# 获取所有子孙标签，会把所有子孙全部拆出来 包括文本内容
# for i in soup.p.descendants:
#     print(i)

# 获取兄弟标签，文本也被当做是一个节点
# 下一个兄弟
# print(soup.a.next_sibling.next_sibling)
# 之后的兄弟们
# print(list(soup.a.next_siblings))

# 上一个兄弟
# print(soup.a.previous_sibling)
# 之前的兄弟们
# print(list(soup.a.previous_siblings))
```

## 搜索文档树

```python
from bs4 import BeautifulSoup
import re

# 要解析的文档内容
html_doc = """
<html>
<head><title>The Dormouse's story</title></head>
<body class="b a c">
<button/>
<abus/>
<p class="story">
<ssss>hhhh</ssss>
Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="1">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
"""

soup = BeautifulSoup(html_doc,'lxml')
# 过滤器，find_all 查找所有匹配的标签
# 按照名字匹配 可以传一个名字或一个列表
# print(soup.find_all('a'))
# print(soup.find_all(['a','p']))

# 找id为link1 的a标签
# print(soup.find_all('a',attrs={'id':'link1'}))
# print(soup.find_all('a',attrs={'class':'sister'}))
# print(soup.find_all(name='a',id='link1'))

# 注意如果要按照条件为class来查找，需要使用class_ 因为class是关键字
# 多个类名加空格即可
# 只能找到类名完全匹配的如:<a class="sister brother">
# print(soup.find_all(name='a',class_='sister brother'))
# 只要类名带有sister就能找到
# print(soup.find_all(name='a',class_='sister'))
# 如果属性带有特殊符号 可以把条件装在attrs中
# print(soup.find_all(name='a',attrs={'data-a':'sister'}))

# 指定文本
# print(soup.find_all(name='a',text='Elsie'))

# 过滤器
# 标签名称中带有a字母的标签
# print(soup.find_all(name="a"))
# res =  re.compile('b')
# 正则匹配
# print(soup.find_all(name=res))

# 数组
# print(soup.find_all(name=['body','a']))

# True表示所有标签
# print(soup.find_all(True))
# 所有具备id属性的标签
# print(soup.find_all(id=True))

# 方法匹配(写个函数来过滤)
# 必须只能有一个参数，参数表示要过滤的标签
def MyFilter(tag):
    return tag.name == "a" and tag.text != "Elsie" and tag.has_attr("id")
print(soup.find_all(MyFilter,limit=1))

# 使用方式和find_all 相同
print(soup.find('a'))

# 总结： 过滤可以是数组，可以是一个 re，可以是一个函数，可以是True
```

### 五种过滤器

```python
#搜索文档树：BeautifulSoup定义了很多搜索方法,这里着重介绍2个: find() 和 find_all() .其它方法的参数和用法类似
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p id="my p" class="title"><b id="bbb" class="boldest">The Dormouse's story</b>
</p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""


from bs4 import BeautifulSoup
soup=BeautifulSoup(html_doc,'lxml')

#1、五种过滤器: 字符串、正则表达式、列表、True、方法
#1.1、字符串：即标签名
print(soup.find_all('b'))

#1.2、正则表达式
import re
print(soup.find_all(re.compile('^b'))) #找出b开头的标签，结果有body和b标签

#1.3、列表：如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有<a>标签和<b>标签:
print(soup.find_all(['a','b']))

#1.4、True：可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点
print(soup.find_all(True))
for tag in soup.find_all(True):
    print(tag.name)

#1.5、方法:如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数 ,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 False
def has_class_but_no_id(tag):
    return tag.has_attr('class') and not tag.has_attr('id')

print(soup.find_all(has_class_but_no_id))
```

### find_all

```python
#2、find_all( name , attrs , recursive , text , **kwargs )
#2.1、name: 搜索name参数的值可以使任一类型的 过滤器 ,字符窜,正则表达式,列表,方法或是 True .
print(soup.find_all(name=re.compile('^t')))

#2.2、keyword: key=value的形式，value可以是过滤器：字符串 , 正则表达式 , 列表, True .
print(soup.find_all(id=re.compile('my')))
print(soup.find_all(href=re.compile('lacie'),id=re.compile('\d'))) #注意类要用class_
print(soup.find_all(id=True)) #查找有id属性的标签

# 有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性:
data_soup = BeautifulSoup('<div data-foo="value">foo!</div>','lxml')
# data_soup.find_all(data-foo="value") #报错：SyntaxError: keyword can't be an expression
# 但是可以通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的tag:
print(data_soup.find_all(attrs={"data-foo": "value"}))
# [<div data-foo="value">foo!</div>]

#2.3、按照类名查找，注意关键字是class_，class_=value,value可以是五种选择器之一
print(soup.find_all('a',class_='sister')) #查找类为sister的a标签
print(soup.find_all('a',class_='sister ssss')) #查找类为sister和sss的a标签，顺序错误也匹配不成功
print(soup.find_all(class_=re.compile('^sis'))) #查找类为sister的所有标签

#2.4、attrs
print(soup.find_all('p',attrs={'class':'story'}))

#2.5、text: 值可以是：字符，列表，True，正则
print(soup.find_all(text='Elsie'))
print(soup.find_all('a',text='Elsie'))

#2.6、limit参数:如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果
print(soup.find_all('a',limit=2))

#2.7、recursive:调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False .
print(soup.html.find_all('a'))
print(soup.html.find_all('a',recursive=False))

'''
像调用 find_all() 一样调用tag
find_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. BeautifulSoup 对象和 tag 对象可以被当作一个方法来使用,这个方法的执行结果与调用这个对象的 find_all() 方法相同,下面两行代码是等价的:
soup.find_all("a")
soup("a")
这两行代码也是等价的:
soup.title.find_all(text=True)
soup.title(text=True)
'''
```

### find

```python
#3、find( name , attrs , recursive , text , **kwargs )
find_all() 方法将返回文档中符合条件的所有tag,尽管有时候我们只想得到一个结果.比如文档中只有一个<body>标签,那么使用 find_all() 方法来查找<body>标签就不太合适, 使用 find_all 方法并设置 limit=1 参数不如直接使用 find() 方法.下面两行代码是等价的:

soup.find_all('title', limit=1)
# [<title>The Dormouse's story</title>]
soup.find('title')
# <title>The Dormouse's story</title>

唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.
find_all() 方法没有找到目标是返回空列表, find() 方法找不到目标时,返回 None .
print(soup.find("nosuchtag"))
# None

soup.head.title 是 tag的名字 方法的简写.这个简写的原理就是多次调用当前tag的 find() 方法:

soup.head.title
# <title>The Dormouse's story</title>
soup.find("head").find("title")
# <title>The Dormouse's story</title>

# recursive 是否递归


```

### CSS选择器

```python
#该模块提供了select方法来支持css,详见官网:https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#id37
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title">
    <b>The Dormouse's story</b>
    Once upon a time there were three little sisters; and their names were
    <a href="http://example.com/elsie" class="sister" id="link1">
        <span>Elsie</span>
    </a>
    <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
    <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
    <div class='panel-1'>
        <ul class='list' id='list-1'>
            <li class='element'>Foo</li>
            <li class='element'>Bar</li>
            <li class='element'>Jay</li>
        </ul>
        <ul class='list list-small' id='list-2'>
            <li class='element'><h1 class='yyyy'>Foo</h1></li>
            <li class='element xxx'>Bar</li>
            <li class='element'>Jay</li>
        </ul>
    </div>
    and they lived at the bottom of a well.
</p>
<p class="story">...</p>
"""
from bs4 import BeautifulSoup
soup=BeautifulSoup(html_doc,'lxml')

#1、CSS选择器
print(soup.p.select('.sister'))
print(soup.select('.sister span'))

print(soup.select('#link1'))
print(soup.select('#link1 span'))

print(soup.select('#list-2 .element.xxx'))

print(soup.select('#list-2')[0].select('.element')) #可以一直select,但其实没必要,一条select就可以了

# 2、获取属性
print(soup.select('#list-2 h1')[0].attrs)

# 3、获取内容
print(soup.select('#list-2 h1')[0].get_text())
```

## bs4 爬取汽车之家新闻

```python
import requests
from bs4 import BeautifulSoup

url = "https://www.autohome.com.cn/news/{page}/"

# 过滤标签
def filter(tag):
    return tag.name =='li' and tag.has_attr("data-artidanchor")

# 获取新闻列表
def get_list_paget(url):
    print(url)
    resp = requests.get(url, headers={
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36"})
    soup = BeautifulSoup(resp.text,'lxml')
    lis = soup.find_all(filter)
    for t in lis:
        print('https:'+t.a.attrs.get('href'))
        print('https:'+t.img.attrs.get('src'))
        print(t.h3.text)
        print(t.span.text)
        print(t.em.text)
        print(t.p.text)

get_list_paget(url.format(page=1))
```

##### 代码补全闭合

```
soup = BeautifulSoup(markup,'lxml')
html_content=soup.prettify()
```

