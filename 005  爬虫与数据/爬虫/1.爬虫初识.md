[TOC]

# 爬虫初识

## 爬虫简介

```python
'''
爬虫是一个用于爬取数据的应用程序
爬取的目标可以是整个互联网，也可以是单独的某一个服务器
在cs结构中 爬虫属于client 客户端
'''
'''
爬虫的价值:
互联网中最有价值的就是数据
爬虫中首要任务就是要通过网络获取模板服务器的数据，来为自己创造价值
'''
```

## 爬虫的流程

```python
1. 分析请求
	web页面(chrome)
    原生软件(抓包工具如Charles)
2. 模拟请求
	第三方requests模块
    内置的urllib模块
    selenium模块(自动化测试模块）用于程序驱动浏览器发送请求
    截获app端发送的请求信息 Charles(青花瓷)
3. 获取响应数据
	浏览器接受相应后会渲染页面进行展示
    requests和urllib都会直接返回响应体
4. 解析数据
	re模块
   	BeautifulSoup模块
5. 存储数据
   文件
   MySQL等关系型数据库
   MongoDB、redis非关系型数据库          
```

## 爬取梨视频

```python
import requests
import re
import json
import os
from concurrent.futures import ThreadPoolExecutor


dir = os.path.dirname(__file__)
# 存储解析完成的数据
datas = []
# 要爬取的页数
page_num = 1
# 要爬取的分类

def get_details(resp):
    res = re.findall('<a href="(video_\d+)"', resp.text)
    base_url = "https://www.pearvideo.com/"
    for i in res:
        # 拼接详情页面的地址
        detail_url = base_url + i
        detail_resp = requests.get(detail_url)
        # 解析标题
        title = re.search('<h1 class="video-tt">(.*?)</h1>', detail_resp.text).group(1)
        # 时间
        subdate = re.search('<div class="date">(.*?)</div>', detail_resp.text).group(1)
        # 点赞数
        f_count = re.search('<div class="fav" data-id="\d+">(\d+)</div>', detail_resp.text).group(1)
        # 作者
        author = re.search('</i>(.*?)</div>', detail_resp.text).group(1)
        # 详情
        content = re.search('<div class="summary">(.*?)</div>', detail_resp.text).group(1)
        # 视频地址
        video_url = re.search('srcUrl="(.*?)"',detail_resp.text).group(1)
        dic = {"title": title, "subdate": subdate, "f_count": f_count, "author": author, "content": content,"video_url":video_url}
        pool.submit(download_video,video_url,title) # 异步提交任务到线程池
        datas.append(dic)

# 请求首页列表
def get_page_data(categoryId):
    url = "https://www.pearvideo.com/category_loading.jsp?reqType=5&categoryId=%s&start=" % categoryId
    for i in range(page_num):
        url1 = url + str(i * 12)
        resp = requests.get(url1)
        if resp.status_code == 200:
            print("请求成功返回！")
            get_details(resp)
            
def download_video(video_url,video_name):
    try:
        print("开始下载",video_name)
        resp = requests.get(video_url)
        video_name = video_name.replace('"',"")
        video_name = video_name.replace('?', "")
        file_path = os.path.join(dir,"videos",video_name+".mp4")
        if os.path.exists(file_path):
            print(video_name,"已经下载过了!")
            return
        with open(file_path,"wb") as f:
            f.write(resp.content)
    except Exception as e:
        print("下载任务执行失败:",e)

# 将数据写入json文件
def write_json():
    with open("datas.json", "wt") as f:
        json.dump(datas, f)

if __name__ == '__main__':
    # 开启线程池
    pool = ThreadPoolExecutor()
    get_page_data(5)
    # 写入
    write_json()
```

## 爬虫流程分析

```python
发送请求 -- 接收响应数据 -- 解析数据 -- 存储数据

1.接收响应 requests
2.解析数据 re bs4 css xpath
3.存储数据 mongodb
selenium模块：自动化测试框架，模拟人的操作
scrapy框架：爬虫框架，可以快速、简单、可扩展的方式从网站中提取所需的数据。
```

