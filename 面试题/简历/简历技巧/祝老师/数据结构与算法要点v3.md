## 数据结构与算法

程序 = 算法 + 数据结构

必胜歌诀：

左**快排**，右**红黑**，**深广搜索**嘴边吹。
上贪心，下动规，心中B树往前推。
还是不行要找谁？记住这句话：
遇事不决，量子力学。还不行，傅立叶。







### 一、基础

1. 大O表示法：大O表示法指出了算法有多快 指出了最糟情况下的运行时间，一些常见的大O运行时间：忽略次要项之后，简化的式子被称为这个程序算法的时间复杂度，记做O(f(n)),f(n)就是简化后的式子，比如说T(n)=3n+3,简化后T(n)~f(n)=n,那我们记为O(n)，准确地说O代表了运行时间函数的一个渐进上界，即T(n)在数量级上小于等于f(n)

    - `O(log n)`，也叫对数时间，这样的算法包括二分查找。
    - `O(n)`，也叫线性时间，这样的算法包括简单查找。
    - `O(nlogn)`，这样的算法包括将介绍的快速排序——一种速度较快的排序算法。
    - `O(n^2)`，这样的算法包括将介绍的选择排序——一种速度较慢的排序算法。
    - `O(n!)`，这样的算法包括将介绍的旅行商问题的解决方案——一种非常慢的算法。

    小总结：

    - 算法的速度指的并非时间，而是操作数的增速。
    - 谈论算法的速度时，说的是随着输入的增加，其运行时间将以什么样的速度增加。
    - O(log n)比O(n)快，当需要搜索的元素越多时，前者比后者快得越多。

2. O(1) < O(logn) <O(n) < O(nlogn) < O(n^2) < O(n^3) < O(2^n) < O(n!) < O(n^n)

3. 如何计算算法复杂度大O

    1. 得出运行时间的函数
    2. 对函数简化
        1. ①用常数1来取代运行时间中所有加法常数 
        2. ②修改后的函数中，只保留最高阶项
        3. ③如果最高阶项存在且不是1，则忽略这个项的系数
    3. 一般来说，最内层执行次数最多的语句就决定了整个算法的趋势。

4. 如下例：

    1. 每循环一次，sum就给自身乘以2，乘了多少次就跳出循环了呢(大于等于n)？不知道，就设为x吧，那么2^x=n,解出x=logn，这说明随着n的增大，最消耗时间的内层语句是呈对数变化的

        ```java
        int sum = 1;
        while (sum < n) {
            sum = sum * 2;
        }
        ```

5. 对于一些简单的输入输出语句或赋值语句,近似认为需要O(1)时间

6. 对于顺序结构,需要依次执行一系列语句所用的时间可采用大O下求和法则

    -   求和法则:是指若算法的2个部分时间复杂度分别为 T1(n)=O(f(n))和 T2(n)=O(g(n)),则 T1(n)+T2(n)=O(max(f(n), g(n)))
    -   特别地,若T1(m)=O(f(m)), T2(n)=O(g(n)),则 T1(m)+T2(n)=O(f(m) + g(n))

7. 对于选择结构,如if语句,它的主要时间耗费是在执行then字句或else字句所用的时间,需注意的是检验条件也需要O(1)时间

8. 对于循环结构,循环语句的运行时间主要体现在多次迭代中执行循环体以及检验循环条件的时间耗费,一般可用大O下乘法法则

    -   乘法法则: 是指若算法的2个部分时间复杂度分别为 T1(n)=O(f(n))和 T2(n)=O(g(n)),则 `T1*T2=O(f(n)*g(n))`

9. 对于复杂的算法,可以将它分成几个容易估算的部分,然后利用求和法则和乘法法则技术整个算法的时间复杂度

    -   若g(n)=O(f(n)),则O(f(n))+ O(g(n))= O(f(n))
    -   O(Cf(n)) = O(f(n)),其中C是一个正常数

10. 递归调用树，一般是等差或等比级数

    ```python
    T(n) = T(n/4) + T(n/2) + cn^2
    
    T(n) = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + ….
    ```

    上述系列是几何级数为5/16。

    为得到一个上限,我们求无穷级数的和： (n2)/(1 – 5/16) 即为 O(n2)

11. 主定理：主定理是解决递归的一种直接方法。但仅用于一些类型或可以转换为以下类型的递归公式：

    ```python
    T(n) = aT(n/b) + f(n)  （a >= 1 且 b > 1）
    ```

### 二、基础数据结构 - Data Structures

1.  线性表
  
    1.  **列表**（必学）
    2.  **链表**（必学）
    3.  跳跃表（知道原理，应用，自己实现一遍） skip-list, 插入比读多的场景可以应用。 牺牲部分读的性能，但是提高并发写的性能。由于元素的有序性，可以通过增加一些路径来加快查找速度。对于具有 n 个元素的链表，我们可以采取 ** (logn + 1) 层指针路径的形式**，**就可以实现在 O(logn) 的时间复杂度内，查找到某个目标元素了。这种数据结构，称为跳跃表，算是链表的一种变形，只是它具有二分查找的功能。
       1.  ![image-20191110201308705](assets/image-20191110201308705.png)
       2.  **新插入的结点应该跨越多少层？**策略是**抛硬币来决定新插入结点跨越的层数**：每次我们要插入一个结点的时候，就来抛硬币，如果抛出来的是**正面**，则继续抛，直到出现**负面**为止，统计这个过程中出现正面的**次数**，这个次数作为结点跨越的层数。通过这种方法，可以尽可能接近理想的层数。
       3.  删除比较简单，直接把对应节点及其所跨越的层数删除就行了。
       4.  跳跃表的每一层都是一条**有序的链表**。
       5.  跳跃表的查找次数近似于层数，时间复杂度为O(logn)，插入、删除也为 O(logn)。
       6.  最底层的链表包含所有元素。
       7.  跳跃表是一种随机化的数据结构(通过抛硬币来决定层数)。
       8.  跳跃表的空间复杂度为 O(n)。
    4.  并查集（建议结合刷题学习）
    
2.  栈与队列

   1.  **栈**（必学）
   2.  **队列**（必学）
   3.  优先队列、堆（必学）
   4.  多级反馈队列（原理与应用）

3.  哈希表（必学）HashMap, TreeMap ，用来内存只读数据的快速检查。Hash是从长数据集到短数据集的映射

   1.  碰撞解决方法：开放定址法、链地址法、再次哈希法、建立公共溢出区（必学）
   2.  布隆过滤器（原理与应用）

4.  树与图：

    1.  二叉树：各种遍历（必学）

    2.  哈夫曼树与编码（原理与应用）

    3.  AVL树（必学）：AVL是严格平衡树，因此在增加或者删除节点的时候，根据不同情况，旋转的次数比红黑树要多；

    4.  1.  右旋：顺时针旋转两个节点，使得父节点R被自己的左孩子Z取代，而R下降成为Z的右孩子，同时原来Z的右孩子成为R的左孩子
       2.  左旋：与右旋反向：逆时针旋转两个节点，使得父节点R被自己的右孩子Y取代，而下降成为Y的左孩子，同时原来Y的左孩子成为R的右孩子
       3.  左-左型：右旋
       4.  右-右型：左旋
       5.  右-左型：先右旋一次，变成右-右型，再左旋
       6.  左-右型：先左旋一次，变成左-左型，再右旋

    5.  **红黑树**（原理与应用）：红黑是用非严格的平衡来换取增删节点时候旋转次数的降低；

       1.  平衡树严格要求每个节点的左子树和右子树的高度差至多等于1，导致频繁插入/删除节点的时候，几乎每次都会破坏平衡树的第二个规则，进而需要通过左旋和右旋来进行调整，这使平衡树的性能大打折扣，为了解决这个问题，于是有了红黑树，红黑树在插入、删除等操作，不会像平衡树那样，频繁着破坏红黑树的规则，所以不需要频繁着调整。
       2.  根节点为黑
       3.  每个叶子节点都是为黑的空节点（NIL）
       4.  任何相邻节点不能同时为红
       5.  每个节点，从该节点到其可到达的叶子节点的所有路径，都包含相同的黑色节点

    6.  B 树与 B+ 树（原理与应用）

       1.  B-Tree

          1.  定义一条数据记录为一个二元组[key, data]，key为记录的键值，对于不同数据记录，key是互不相同的；data为数据记录除key外的数据。那么B-Tree是满足下列条件的数据结构：
             1.  d为大于1的一个正整数，称为B-Tree的度。
             2.  h为一个正整数，称为B-Tree的高度。
             3.  每个非叶子节点由n-1个key和n个指针组成，其中d<=n<=2d。
             4.  每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。
             5.  所有叶节点具有相同的深度，等于树高h。
             6.  key和指针互相间隔，节点两端是指针。
             7.  一个节点中的key从左到右非递减排列。
             8.  所有节点组成树结构。
             9.  每个指针要么为null，要么指向另外一个节点。
             10.  如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key1)v(key1)，其中v(key1)v(key1)为node的第一个key的值。
             11.  如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(keym)v(keym)，其中v(keym)v(keym)为node的最后一个key的值。
             12.  如果某个指针在节点node的左右相邻key分别是keyikeyi和keyi+1keyi+1且不为null，则其指向节点的所有key小于v(keyi+1)v(keyi+1)且大于v(keyi)v(keyi)。
          2.  d=2 的 B-Tree 示意图。
             1.  ![img](http://blog.codinglabs.org/uploads/pictures/theory-of-mysql-index/2.png)
             2.  由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。
             3.  一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)logd((N+1)/2)O(logdN)O(logdN)
             4.  由于插入删除新的数据记录会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质。

       2.  与B-Tree相比，B+Tree有以下不同点：

          1.  每个节点的指针上限为2d而不是2d+1。
          2.  内节点不存储data，只存储key；叶子节点不存储指针。
          3.  B+Tree示意。![img](http://blog.codinglabs.org/uploads/pictures/theory-of-mysql-index/3.png)
          4.  由于并不是所有节点都具有相同的域，因此B+Tree中叶节点和内节点一般大小不同。
          5.  这点与B-Tree不同，虽然B-Tree中不同节点存放的key和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中B-Tree往往对每个节点申请同等大小的空间。
          6.  一般来说，B+Tree比B-Tree更适合实现外存储索引结构。

       3.  带有顺序访问指针的B+Tree

          1.  一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。
          2.  
          3.  ![img](http://blog.codinglabs.org/uploads/pictures/theory-of-mysql-index/4.png)
          4.  在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。

       4.  索引的效率：

          1.  一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。

          2.  磁盘读取的局部性原理：磁盘本身存取比内存慢很多，磁盘的存取速度往往是内存的几百分之一，因此为了提高效率，要尽量减少磁盘I/O。即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。

          3.  根据B-Tree的定义，检索一次最多需要访问h个节点。利用磁盘预读原理，将一个节点的大小设为等于一页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一页空间，这样就保证一个节点物理上也存储在一页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。

          4.  B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。综上，用B-Tree作为索引结构效率非常高。

          5.  而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。

          6.  B+Tree更适合外存索引，原因和内节点出度d有关。从上面分析可以看到，d越大索引的性能越好，而出度的上限取决于节点内key和data的大小：

             ```python
             dmax=floor(pagesize/(keysize+datasize+pointsize))dmax=floor(pagesize/(keysize+datasize+pointsize))
             ```

          7.  floor表示向下取整。由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。

       5.  MyISAM引擎

          1.  使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图：

          2.  ![img](http://blog.codinglabs.org/uploads/pictures/theory-of-mysql-index/8.png)

          3.  MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。MyISAM的索引方式也叫做“非聚集”的，与InnoDB的聚集索引区分开。

          4.  InnoDB的数据文件本身就是索引文件。MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。叶节点包含了完整的数据记录。这种索引叫做聚集索引。

          5.  InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。

          6.  知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。

          7.  MySQL索引优化

             1.  前缀索引：就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。前缀索引兼顾索引大小和查询速度，但是其缺点是不能用于ORDER BY和GROUP BY操作，也不能用于Covering index（即当索引本身包含查询所需全部数据时，不再访问数据文件本身）

             2.  ```sql
                ALTER TABLE db_name.tbl_name ADD INDEX `index_name` (first_col, second_col(4));
                ```

          8.  在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。

    7.  前缀树（原理与应用）

       1.  字典树、Trie、单词查找树：共享**字符串的公共前缀**来达到节省空间的目的。
          1.  trie 树的**根节点**不存任何数据，每**整个**个分支代表一个完整的字符串。
          2.  每个分支的内部可能也含有完整的字符串，所以我们可以对于那些是某个字符串结尾的节点做一个**标记**

    8.  线段树（原理与应用）特殊场景用到，优化经常性的区间计算

    9.  并查集 Union Find

    10.  LSM-tree : K-V 数据库的核心数据结构

5.  数组

   1.  树状数组
   2.  矩阵（必学）

6.  堆：大 / 小根堆、可并堆、二叉堆

    1.  小顶堆：
        1.  插入：插入的时候，先把新节点插到完全二叉树的最后一个位置。之后我们再来进行调整，调整的原则是上浮策略：让新插入的节点与它的父节点进行比较，如果新节点小于父节点，则让新节点上浮，即和父节点交换位置。上浮之后继续和它的父节点进行比较，直到父节点的值小于或等于该节点，才停止上浮，即插入结束。
        2.  删除：一般删除的是根节点，把根节点删除之后，用二叉堆的最后一个元素顶替上来，然后在进行调整恢复。调整的原则是下沉策略：和左右孩子节点相比较，如果左右节点有小于当前节点的，则让那个较小的孩子代替当前节点的位置，然后当前节点下沉。直到没有比当前节点小的孩子，或没有左右孩子为止。
        3.  构建：要把一个无序的完全二叉树调整为二叉堆，我们可以让所有非叶子节点依次下沉。不过下沉的顺序不是从根节点开始下沉(想一下相必你就 知道不能从根节点开始下沉)，而是从下面的非叶子节点下称，在依次往上。

7.  有限状态机

8.  有用的 Python 库

   - **heapq**: 基于list的小顶堆，
     - heappush(heap,item)
     - heappop(heap)
     - heapify(x)
     - nlagest(n,iterable[,key])
     - nsmallest(n, iterable[, key])
   - **bisect**: 基于list的有序序列
     - bisect_left(list, item[, lo[, hi]])
     - bisect(...)
     - insort_left(list, item[, lo[, hi]])
     - insort(...)
   - **deque**: from collections import deque
     - deque([iterable])
     - append(x)
     - appendleft(x)
     - extend(it)
     - extendleft(it)
     - pop()
     - popleft()
     - clear()
     - remove(x)
     - rotate(n)

### 三、算法 - Algorithms

1. 排序算法：![image-20191110204108129](assets/image-20191110204108129.png)

    1. 简单排序：插入排序、选择排序、冒泡排序（必学）
        1. 插入排序：从数组第2个元素开始抽取元素。把它与左边第一个元素比较，如果左边第一个元素比它大，则继续与左边第二个元素比较下去，直到遇到不比它大的元素，然后插到这个元素的右边。继续选取第3，4，….n个元素,重复步骤 2 ，选择适当的位置插入。
        2. 选择排序：首先，找到数组中最小的那个元素，其次，将它和数组的第一个元素交换位置(如果第一个元素就是最小元素那么它就和自己交换)。其次，在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置。如此往复，直到将整个数组排序。
        3. 冒泡排序：把第一个元素与第二个元素比较，如果第一个比第二个大，则交换他们的位置。接着继续比较第二个与第三个元素，如果第二个比第三个大，则交换他们的位置….我们对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样一趟比较交换下来之后，排在最右的元素就会是最大的数。除去最右的元素，我们对剩余的元素做同样的工作，如此重复下去，直到排序完成。
    2. 分治排序：快速排序、归并排序（必学，快速排序还要关注中轴的选取方式）
        1. 归并排序：通过递归的方式将大的数组一直分割，直到数组的大小为 1，此时只有一个元素，那么该数组就是有序的了，之后再把两个数组大小为1的合并成一个大小为2的，再把两个大小为2的合并成4的 ….. 直到全部小的数组合并起来。
        2. **快速排序**：我们从数组中随机选择一个元素，称为**中轴元素**，然后把数组中所有小于中轴元素的放在其左边，所有大于或等于中轴元素的放在其右边，此时中轴元素所处的位置是**有序的**。无需再移动中轴元素。从中轴元素那里开始把数组切割成两个小的数组(都不包含中轴元素)，接着我们通过递归的方式，让中轴元素左边的数组和右边的数组也重复同样的操作，直到数组的大小为1，此时每个元素都处于**有序的位置**。
    3. 分配排序：桶排序、基数排序
        1. 桶排序：就是把最大值和最小值之间的数进行瓜分，例如分成  10 个区间，10个区间对应10个桶，我们把各元素放到对应区间的桶中去，再对每个桶中的数进行排序，可以采用归并排序，也可以采用快速排序之类的。之后每个桶里面的数据就是有序的了，我们在进行合并汇总。
        2. 基数排序：先以个位数的大小来对数据进行排序，接着以十位数的大小来多数进行排序，接着以百位数的大小……排到最后，就是一组有序的元素了。不过，在以某位数进行排序的时候，是用“桶”来排序的。由于某位数（个位/十位….，不是一整个数）的大小范围为0-9，所以我们需要10个桶，然后把具有相同数值的数放进同一个桶里，之后再把桶里的数按照0号桶到9号桶的顺序取出来，这样一趟下来，按照某位数的排序就完成了
    4. 树状排序：堆排序（必学）(python 的 sort() 用的是堆排序)
        1. 堆排序：堆的特点就是堆顶的元素是一个最值（最大或最小），大顶堆的堆顶是最大值，小顶堆的堆顶是最小值。堆排序就是把堆顶的元素与最后一个元素交换，交换之后破坏了堆的特性，我们再把堆中剩余的元素再次构成一个大顶堆，然后再把堆顶元素与最后第二个元素交换….如此往复下去，等到剩余的元素只有一个的时候，此时的数组就是有序的了。
    5. 其他：计数排序（必学）、希尔排序
        1. 计数排序：就是把数组元素作为数组的下标，然后用一个临时数组统计该元素出现的次数，例如 temp[i] = m, 表示元素 i 一共出现了 m 次。最后再把临时数组统计的数据从小到大汇总起来，此时汇总起来是数据是有序的。
        2. 希尔排序：本质是分治法，是插入排序的一种变种。希尔排序的思想是采用插入排序的方法，先让数组中任意间隔为 h 的元素有序，刚开始 h 的大小可以是 h = n / 2,接着让 h = n / 4，让 h 一直缩小，当 h = 1 时，也就是此时数组中任意间隔为1的元素有序，此时的数组就是有序的了。
    6. 搞笑排序算法：
        1. 猴子排序，面条排序，睡眠排序。

2.  图算法

    1.  图的表示：邻接矩阵和邻接表

        1. 邻接矩阵：顶点和顶点之间有边关联，那么矩阵中的对应元素的值就是1，矩阵从左上到右下的一条对角线，其上的元素值必然是0：任何一个顶点与它自身是没有连接的。
        2. 邻接表：图的每一个顶点都是一个链表的头节点，其后连接着该顶点能够直接达到的相邻顶点。
        3. 逆邻接表：逆邻接表每一个顶点作为链表的头节点，后继节点所存储的是能够直接达到该顶点的相邻顶点。
        4. 十字链表：十字链表的每一个顶点，都是两个链表的根节点，其中一个链表存储着该顶点能到达的相邻顶点，另一个链表存储着能到达该顶点的相邻节点。优化之后的十字链表中，链表的每一个节点不再是顶点，而是一条边，里面包含起止顶点的下标。

    2. **遍历算法**：深度搜索和广度搜索(必学)

        1. 深度优先搜索。深度优先搜索的步骤为：访问图的一个顶点v，然后依次从v的未被访问的邻接点出发，对图进行深度优先遍历，直至图中和v有路径相通的顶点都被访问。如果此时图中尚有顶点未被访问，则从一个未被访问过的顶点出发，重新进行深度优先遍历，直到图中所有顶点均被访问过为止。

            1. DFS沿着树的深度遍历树的节点，尽可能深的搜索树的分 支。当节点v的所有边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点为止。如果还存在未被发 现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止。利用深度优先搜索算法可以产生目标图的相应拓扑排序表，利用拓扑排序表可以方便的解决很多相关的图论问题，如最大路径问题等等。一般用堆数据结构来辅助实现DFS算法。
            2. 步骤：

                1. 访问顶点v；

                2. 依次从v的未被访问的邻接点出发，对图进行深度优先遍历；直至图中和v有路径相通的顶点都被访问；

                3. 若此时图中尚有顶点未被访问，则从一个未被访问的顶点出发，重新进行深度优先遍历，直到图中所有顶点均被访问过为止。

        2.  广度优先搜索。广度优先搜索从图的一个顶点v出发，访问v所有未被访问的邻接点。然后依次从这些邻接点出发，访问它们所有未被访问的邻接点。以此类推，直到途中所有访问过的邻接点都被访问到。

            1.  BFS是从根节点开始，沿着树(图)的宽度遍历树(图)的节点。如果所有节点均被访问，则算法中止。BFS同样属于盲目搜索。一般用队列数据结构来辅助实现BFS算法。

            2.  步骤：

                1. 首先将根节点放入队列中。

                2. 从队列中取出第一个节点，并检验它是否为目标。如果找到目标，则结束搜寻并回传结果。否则将它所有尚未检验过的直接子节点加入队列中。

                3. 若队列为空，表示整张图都检查过了——亦即图中没有欲搜寻的目标。结束搜寻并回传“找不到目标”。

                4. 重复步骤2。

        3. 作业：Python代码实现的深度优先和广度优先搜索

    3. 最短路径算法：Floyd，Dijkstra（必学）

        1. Floyd：带权图多源点之间的最短路径的动态规划
        2. Dijkstra：单源最短路径算法，本质是不断刷新起点与其他各个顶点之间的 “距离表”。
            1. 算法步骤：
                1. 初始时令 S={V0},T={其余顶点}，T中顶点对应的距离值，若存在<v0,vi>，d(V0,Vi)为<v0,vi>弧上的权值，若不存在<v0,vi>，d(V0,Vi)为∞
                2. 从T中选取一个其距离值为最小的顶点W且不在S中，加入S
                3. 对其余T中顶点的距离值进行修改：若加进W作中间顶点，从V0到Vi的距离值缩短，则修改此距离值
                4. 重复上述步骤2、3，直到S中包含所有顶点，即W=Vi为止

    4. 最小生成树算法：Prim，Kruskal（必学）

    5. 实际常用算法：关键路径、拓扑排序（原理与应用）

    6. 二分图匹配：配对、匈牙利算法（原理与应用）

    7. 拓展：中心性算法、社区发现算法（原理与应用）

3.  搜索与回溯算法：回溯、递归、剪枝技巧

    1.  贪心算法（必学）

    2. 启发式搜索算法：A*寻路算法（了解）

    3. 地图着色算法、N 皇后问题、最优加工顺序

    4. 旅行商问题

    5.  顺序查找。从线性表的一端开始，顺序扫描，将给定的元素依次与扫描到的元素进行比较，直到找出与给定元素相同的元素，或将线性表中的元素都与其比较完为止。

    6.  BFPRT(线性查找算法)：从某n个元素的序列中选出第k大（第k小）的元素，通过巧妙的分 析，BFPRT可以保证在最坏情况下仍为线性时间复杂度。该算法的思想与快速排序思想相似，当然，为使得算法在最坏情况下，依然能达到o(n)的时间复杂 度，五位算法作者做了精妙的处理。

        算法步骤：

        1.  将n个元素每5个一组，分成n/5(上界)组。
        2.  取出每一组的中位数，任意排序方法，比如插入排序。
        3.  递归的调用selection算法查找上一步中所有中位数的中位数，设为x，偶数个中位数的情况下设定为选取中间小的一个。
        4.  用x来分割数组，设小于等于x的个数为k，大于x的个数即为n-k。
        5.  若i==k，返回x；若i<k，在小于x的元素中递归查找第i小的元素；若i>k，在大于x的元素中递归查找第i-k小的元素。

        终止条件：n=1时，返回的即是i小元素。

    7.  **二分查找**。二分查找算法是一种在有序数组中查找某一特定元素的搜索算法。从数组的中间元素开始，如果中间元素正好是要查找的元素，则结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。

    8.  二叉查找树。在二叉查找树中使用递归算法查找元素：如果树是空的，则查找失败；如果待查找元素和根结点的元素相同，查找成功，否则就（递归地）在适当的子树中继续查找。如果待查找元素比较小就选择左子树，较大则选择右子树。

    9.  散列表。使用三列的查找算法分为两步。第一步是利用散列函数将待查找元素转化为数组的一个索引，但有时会出现两个或多个元素散列到相同的索引值的情况。因此，散列查找的第二部是处理碰撞冲突，主要有拉链法和线性探测法两种方法。

4.  网络流算法

    1. 最大流：最短增广路、Dinic 算法
    2. 最大流最小割：最大收益问题、方格取数问题
    3. 最小费用最大流：最小费用路、消遣
    4. 最大流的增广路算法(KM算法)

5.  动态规划：背包问题、最长子序列、计数问题

    1. 动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。 通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量： 一旦某个给定子问题的解已经算出 ，则将其记忆化存储，以便下次需要同一个 子问题解之时直接查表。 这种做法在重复子问题的数目关于输入的规模呈指数增长时特别有用。
    2. 树形DP：
        1. 背包问题算法步骤：
            1. 最优子结构性质：如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态规划算法解决问题提供了最重要线索。
            2. 子问题重叠性质是指在用递归算法自顶向下对问题求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。 动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只在表格中简单地查看一下结果，从而获得较高的效率。
    3. 线性DP：最长公共子序列、最长公共子串
    4. 区间DP：矩阵最大值（和以及积）
    5. 数位DP：数字游戏
    6. 状态压缩DP：旅行商

6.  字符串：字典树、前缀树、后缀树，前缀树在工程中应用比较多。 提高字符串的查询速度。

    1.  正则表达式

    2. 模式匹配：KMP、Boyer-Moore

    3.  KMP：KMP 算法用来解决字符串查找的问题，可以在一个字符串（S）中查找一个子串（W）出现的位置。KMP 算法把字符匹配的时间复杂度缩小到 O(m+n) ,而空间复杂度也只有O(m)。因为“暴力搜索”的方法会反复回溯主串，导致效率低下，而KMP算法可以利用已经部分匹配这个有效信息，保持主串上的指针不回溯，通过修改子串的指针，让模式串尽量地移动到有效的位置。KMP算法解释。

        1.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050103.png)

        首先，字符串"BBC ABCDAB ABCDABCDABDE"的第一个字符与搜索词"ABCDABD"的第一个字符，进行比较。因为B与A不匹配，所以搜索词后移一位。

        2.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050104.png)

        因为B与A不匹配，搜索词再往后移。

        3.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050105.png)

        就这样，直到字符串有一个字符，与搜索词的第一个字符相同为止。

        4.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050106.png)

        接着比较字符串和搜索词的下一个字符，还是相同。

        5.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png)

        直到字符串有一个字符，与搜索词对应的字符不相同为止。

        6.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050108.png)

        这时，最自然的反应是，将搜索词整个后移一位，再从头逐个比较。这样做虽然可行，但是效率很差，因为你要把"搜索位置"移到已经比较过的位置，重比一遍。

        7.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png)

        一个基本事实是，当空格与D不匹配时，你其实知道前面六个字符是"ABCDAB"。KMP算法的想法是，设法利用这个已知信息，不要把"搜索位置"移回已经比较过的位置，继续把它向后移，这样就提高了效率。

        8.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050109.png)

        怎么做到这一点呢？可以针对搜索词，算出一张《部分匹配表》（Partial Match Table）。这张表是如何产生的，后面再介绍，这里只要会用就可以了。

        9.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png)

        已知空格与D不匹配时，前面六个字符"ABCDAB"是匹配的。查表可知，最后一个匹配字符B对应的"部分匹配值"为2，因此按照下面的公式算出向后移动的位数：

        > 移动位数 = 已匹配的字符数 - 对应的部分匹配值

        因为 6 - 2 等于4，所以将搜索词向后移动4位。

        10. 

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050110.png)

        因为空格与Ｃ不匹配，搜索词还要继续往后移。这时，已匹配的字符数为2（"AB"），对应的"部分匹配值"为0。所以，移动位数 = 2 - 0，结果为 2，于是将搜索词向后移2位。

        11.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050111.png)

        因为空格与A不匹配，继续后移一位。

        12.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050112.png)

        逐位比较，直到发现C与D不匹配。于是，移动位数 = 6 - 2，继续将搜索词向后移动4位。

        13.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050113.png)

        逐位比较，直到搜索词的最后一位，发现完全匹配，于是搜索完成。如果还要继续搜索（即找出全部匹配），移动位数 = 7 - 0，再将搜索词向后移动7位，这里就不再重复了。

        14.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050114.png)

        下面介绍《部分匹配表》是如何产生的。

        首先，要了解两个概念："前缀"和"后缀"。 "前缀"指除了最后一个字符以外，一个字符串的全部头部组合；"后缀"指除了第一个字符以外，一个字符串的全部尾部组合。

        15.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050109.png)

        "部分匹配值"就是"前缀"和"后缀"的最长的共有元素的长度。以"ABCDABD"为例，

        > －　"A"的前缀和后缀都为空集，共有元素的长度为0；
        >
        > －　"AB"的前缀为[A]，后缀为[B]，共有元素的长度为0；
        >
        > －　"ABC"的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；
        >
        > －　"ABCD"的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；
        >
        > －　"ABCDA"的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为"A"，长度为1；
        >
        > －　"ABCDAB"的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为"AB"，长度为2；
        >
        > －　"ABCDABD"的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。

        16.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050112.png)

        "部分匹配"的实质是，有时候，字符串头部和尾部会有重复。比如，"ABCDAB"之中有两个"AB"，那么它的"部分匹配值"就是2（"AB"的长度）。搜索词移动的时候，第一个"AB"向后移动4位（字符串长度-部分匹配值），就可以来到第二个"AB"的位置。

    4.  BM算法也是一种精确字符串匹配算法，它采用从右向左比较的方法，同时应用到了两种启发式规则，即坏字符规则 和好后缀规则 ，来决定向右跳跃的距离。基本思路就是从右往左进行字符匹配，遇到不匹配的字符后从坏字符表和好后缀表找一个最大的右移值，将模式串右移继续匹配。 Boyer-Moore算法不仅效率高，而且构思巧妙，容易理解。1977年，德克萨斯大学的Robert S. Boyer教授和J Strother Moore教授发明了这种算法。

        下面，我根据Moore教授自己的[例子](http://www.cs.utexas.edu/~moore/best-ideas/string-searching/fstrpos-example.html)来解释这种算法。

        1.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050302.png)

        假定字符串为"HERE IS A SIMPLE EXAMPLE"，搜索词为"EXAMPLE"。

        2.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050303.png)

        首先，"字符串"与"搜索词"头部对齐，从尾部开始比较。

        这是一个很聪明的想法，因为如果尾部字符不匹配，那么只要一次比较，就可以知道前7个字符（整体上）肯定不是要找的结果。

        我们看到，"S"与"E"不匹配。这时，**"S"就被称为"坏字符"（bad character），即不匹配的字符。**我们还发现，"S"不包含在搜索词"EXAMPLE"之中，这意味着可以把搜索词直接移到"S"的后一位。

        3.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050304.png)

        依然从尾部开始比较，发现"P"与"E"不匹配，所以"P"是"坏字符"。但是，"P"包含在搜索词"EXAMPLE"之中。所以，将搜索词后移两位，两个"P"对齐。

        4.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050305.png)

        我们由此总结出"坏字符规则"：

        > 后移位数 = 坏字符的位置 - 搜索词中的上一次出现位置

        如果"坏字符"不包含在搜索词之中，则上一次出现位置为 -1。

        以"P"为例，它作为"坏字符"，出现在搜索词的第6位（从0开始编号），在搜索词中的上一次出现位置为4，所以后移 6 - 4 = 2位。再以前面第二步的"S"为例，它出现在第6位，上一次出现位置是 -1（即未出现），则整个搜索词后移 6 - (-1) = 7位。

        5.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050306.png)

        依然从尾部开始比较，"E"与"E"匹配。

        6.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050307.png)

        比较前面一位，"LE"与"LE"匹配。

        7.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050308.png)

        比较前面一位，"PLE"与"PLE"匹配。

        8.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050309.png)

        比较前面一位，"MPLE"与"MPLE"匹配。**我们把这种情况称为"好后缀"（good suffix），即所有尾部匹配的字符串。**注意，"MPLE"、"PLE"、"LE"、"E"都是好后缀。

        9.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050310.png)

        比较前一位，发现"I"与"A"不匹配。所以，"I"是"坏字符"。

        10.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050311.png)

        根据"坏字符规则"，此时搜索词应该后移 2 - （-1）= 3 位。问题是，此时有没有更好的移法？

        11.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050309.png)

        我们知道，此时存在"好后缀"。所以，可以采用**"好后缀规则"**：

        > 后移位数 = 好后缀的位置 - 搜索词中的上一次出现位置

        举例来说，如果字符串"ABCDAB"的后一个"AB"是"好后缀"。那么它的位置是5（从0开始计算，取最后的"B"的值），在"搜索词中的上一次出现位置"是1（第一个"B"的位置），所以后移 5 - 1 = 4位，前一个"AB"移到后一个"AB"的位置。

        再举一个例子，如果字符串"ABCDEF"的"EF"是好后缀，则"EF"的位置是5 ，上一次出现的位置是 -1（即未出现），所以后移 5 - (-1) = 6位，即整个字符串移到"F"的后一位。

        这个规则有三个注意点：

        > （1）"好后缀"的位置以最后一个字符为准。假定"ABCDEF"的"EF"是好后缀，则它的位置以"F"为准，即5（从0开始计算）。
        >
        > （2）如果"好后缀"在搜索词中只出现一次，则它的上一次出现位置为 -1。比如，"EF"在"ABCDEF"之中只出现一次，则它的上一次出现位置为-1（即未出现）。
        >
        > （3）如果"好后缀"有多个，则除了最长的那个"好后缀"，其他"好后缀"的上一次出现位置必须在头部。比如，假定"BABCDAB"的"好后缀"是"DAB"、"AB"、"B"，请问这时"好后缀"的上一次出现位置是什么？回答是，此时采用的好后缀是"B"，它的上一次出现位置是头部，即第0位。这个规则也可以这样表达：如果最长的那个"好后缀"只出现一次，则可以把搜索词改写成如下形式进行位置计算"(DA)BABCDAB"，即虚拟加入最前面的"DA"。

        回到上文的这个例子。此时，所有的"好后缀"（MPLE、PLE、LE、E）之中，只有"E"在"EXAMPLE"还出现在头部，所以后移 6 - 0 = 6位。 

        12.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050312.png)

        可以看到，"坏字符规则"只能移3位，"好后缀规则"可以移6位。所以，Boyer-Moore算法的基本思想是，每次后移这两个规则之中的较大值。

        更巧妙的是，这两个规则的移动位数，只与搜索词有关，与原字符串无关。因此，可以预先计算生成《坏字符规则表》和《好后缀规则表》。使用时，只要查表比较一下就可以了。

        13.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050313.png)

        继续从尾部开始比较，"P"与"E"不匹配，因此"P"是"坏字符"。根据"坏字符规则"，后移 6 - 4 = 2位。

        14.

        ![](http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050314.png)

        从尾部开始逐位比较，发现全部匹配，于是搜索结束。如果还要继续查找（即找出全部匹配），则根据"好后缀规则"，后移 6 - 0 = 6位，即头部的"E"移到尾部的"E"的位置。

7.  基础技巧：分治、倍增、二分、贪心

    1.  分治 Divide Conquer 
    2.  回溯法 Backtracking 
    3.  双指针 Two Pointers 
    4.  扫描线 Scan-line algorithm

8.  分布式算法：

    1.  Hash
    2.  一致hash

    - Paxos 算法
        - Paxos算法基于投票机制，一条订单需得到超过半数节点的赞成投票才能被全局选定。
        - 协议中根据职责划分了角色
            - Client：发送订单请求
            - Proposer：发起对订单的投票（只保留一个，称为Leader）
            - Acceptor：响应对订单的投票
            - Learner：学习订单投票的结果
        - 基本概念：
            - 提案值(Proposal): 候选订单
            - 决议(Agreed Value)：已选定订单
            - 批准（Accept）：投票赞成
            - 通过（Choose）：选定某个订单，不允许再修改
        - Proposer与Acceptor选定订单的过程，需通过一个两阶段的协议进行：
            - Prepare阶段：为即将发起的投票收集信息，避免发起投票的订单值与之前已通过的订单值冲突
            - Accept阶段：正式进行投票，并收集投票结果，形成决议
        - Learner需要对投票结果进行学习，获知决议
        - Paxos算法适合环境
            - 分布式系统故障分类
                - 拜占庭故障（恶性故障）
                - 非拜占庭故障（良性故障）
            - 处理拜占庭故障代价较高，证券交易系统采用专网，发生恶性故障概率低
            - 基本的Paxos算法，适用于非拜占庭故障
                - 处理器：以任意速度运行，可能故障，但不会串通、说谎以及进行让协议转向的尝试
                - 网络：消息可能花费任意长的时间到达，可能丢失、乱序或重复传送，但不会篡改。
        - 一致性证明：
            - Paxos算法核心在于通过两个多数集合至少有一个公共结点的性质来保证一致性
                - 一个订单Va被选定后，则有超过半数的结点在Accept阶段对其投过赞成票
                - 若想选定另一订单Vb，必须先在Prepare阶段收集到超过半数的响应，其中必然包含Va，从而使得Accept阶段只能发送Va
            - Paxos算法用2N+1个结点容忍最多N个结点发生非拜占庭故障，保证数据的一致性。

### 三、统计分析

1. **机器学习算法**
   1. k-近邻算法。k-近邻算法通过测量不同特征值之间的距离进行分类。当输入没有标签的新数据时，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似（最近邻）数据的分类标签。
   2. 决策树。原理类似于“20个问题”游戏：参与游戏的一方在脑海里想某个事物，其他参与者向他提问题，只允许提20个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围。在决策树算法中，用户输入一系列数据，然后给出决策。
   3. 朴素贝叶斯。朴素贝叶斯是一种基于概率论的分类算法，在做决策时要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。
   4. 支持向量机。支持向量机是一种监督学习方法，是一种对数据进行二元分类的线性分类器。它通过对学习样本求解最大边距超平面来进行分类。支持向量机的各种衍生算法已经实现了多分类、回归等功能，在模式识别领域有广泛应用。
   5. AdaBoost元算法。AdaBoost算法是一种迭代算法，其原理是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个性能更强的分类器（强分类器）。
   6. k均值聚类算法。这是一种比较常用的聚类方法。首先，确定k个初始点作为质心。然后将数据集中的每个点分配到一个簇中。具体来讲，为每个点找距其最近的质心，并将其分配给该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。
   7. 主成分分析（PCA）。这是一种经典的降维方法。将一个矩阵中的样本数据投影到一个新的空间中，将原来多个变量的复杂因素归结成为几个主要成分，使问题简单化，得到的结果更加科学有效。

2. 深度学习
   1. **卷积神经网络**。卷积神经网络是基于人类视皮层中感受野的结构得到的模型，由输入层、卷积层、池化层、全连接层和输出层组成。通过增加卷积层和池化层，还可以得到更深层次的网络。卷积神经网络可用于语音识别和图像识别等领域。
   2. **受限玻尔兹曼机**。玻尔兹曼机起源于Hopfield这种相互连接型神经网络，在此基础上又提出了受限玻尔兹曼机。受限玻尔兹曼机是由可见层和隐藏层构成的，可见层和隐藏层又分别由可见变量和隐藏变量构成。它可以用于降维、分类、回归、协同过滤、特征学习以及主题建模的算法。
   3. **自编码器**。自编码器是一种有效地数据维度压缩算法，目的在于通过不断调整参数，重构经过维度压缩的样本，常用于数据降噪和降维。

